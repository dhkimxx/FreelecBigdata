{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "howToFindPythonKeyWords.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMfrmhuM8vEFcXYc5Yoat+x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhkimxx/bigData-analysis-examples/blob/main/howToFindPythonKeyWords.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bw5NOZ-WUHx2",
        "outputId": "5de6784f-a55d-4538-8fc0-038b60c216b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__SKLEARN_SETUP__', '__all__', '__builtins__', '__cached__', '__check_build', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_config', '_distributor_init', 'base', 'clone', 'config_context', 'exceptions', 'externals', 'get_config', 'logger', 'logging', 'os', 'preprocessing', 'random', 'set_config', 'setup_module', 'show_versions', 'sys', 'utils']\n"
          ]
        }
      ],
      "source": [
        "import sklearn\n",
        "print(dir(sklearn)) \n",
        "#sklearn 패키지 모듈 확인\n",
        "#preprocessing, metrics 왜 안나올까?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.preprocessing\n",
        "print(dir(sklearn.preprocessing))\n",
        "#sklearn.preprocessing 모듈 하위의 함수명 확인\n",
        "#LabelEncoder, MinMaxScaler, OneHotEncoder, RobustScaler, StandardScaler 등 함수명 확인"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3ZEdL-lZRKd",
        "outputId": "bffd217f-00c2-437d-cc75-70c945034b3c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Binarizer', 'FunctionTransformer', 'KBinsDiscretizer', 'KernelCenterer', 'LabelBinarizer', 'LabelEncoder', 'MaxAbsScaler', 'MinMaxScaler', 'MultiLabelBinarizer', 'Normalizer', 'OneHotEncoder', 'OrdinalEncoder', 'PolynomialFeatures', 'PowerTransformer', 'QuantileTransformer', 'RobustScaler', 'SplineTransformer', 'StandardScaler', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_csr_polynomial_expansion', '_data', '_discretization', '_encoders', '_function_transformer', '_label', '_polynomial', 'add_dummy_feature', 'binarize', 'label_binarize', 'maxabs_scale', 'minmax_scale', 'normalize', 'power_transform', 'quantile_transform', 'robust_scale', 'scale']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "print(help(StandardScaler))\n",
        "#StandardScaler 함수안의 파라미터 확인"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ou5IQZuUb2RC",
        "outputId": "087d32fb-35a8-4596-d0b7-70a055d6c18b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class StandardScaler in module sklearn.preprocessing._data:\n",
            "\n",
            "class StandardScaler(sklearn.base._OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
            " |  StandardScaler(*, copy=True, with_mean=True, with_std=True)\n",
            " |  \n",
            " |  Standardize features by removing the mean and scaling to unit variance.\n",
            " |  \n",
            " |  The standard score of a sample `x` is calculated as:\n",
            " |  \n",
            " |      z = (x - u) / s\n",
            " |  \n",
            " |  where `u` is the mean of the training samples or zero if `with_mean=False`,\n",
            " |  and `s` is the standard deviation of the training samples or one if\n",
            " |  `with_std=False`.\n",
            " |  \n",
            " |  Centering and scaling happen independently on each feature by computing\n",
            " |  the relevant statistics on the samples in the training set. Mean and\n",
            " |  standard deviation are then stored to be used on later data using\n",
            " |  :meth:`transform`.\n",
            " |  \n",
            " |  Standardization of a dataset is a common requirement for many\n",
            " |  machine learning estimators: they might behave badly if the\n",
            " |  individual features do not more or less look like standard normally\n",
            " |  distributed data (e.g. Gaussian with 0 mean and unit variance).\n",
            " |  \n",
            " |  For instance many elements used in the objective function of\n",
            " |  a learning algorithm (such as the RBF kernel of Support Vector\n",
            " |  Machines or the L1 and L2 regularizers of linear models) assume that\n",
            " |  all features are centered around 0 and have variance in the same\n",
            " |  order. If a feature has a variance that is orders of magnitude larger\n",
            " |  that others, it might dominate the objective function and make the\n",
            " |  estimator unable to learn from other features correctly as expected.\n",
            " |  \n",
            " |  This scaler can also be applied to sparse CSR or CSC matrices by passing\n",
            " |  `with_mean=False` to avoid breaking the sparsity structure of the data.\n",
            " |  \n",
            " |  Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  copy : bool, default=True\n",
            " |      If False, try to avoid a copy and do inplace scaling instead.\n",
            " |      This is not guaranteed to always work inplace; e.g. if the data is\n",
            " |      not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n",
            " |      returned.\n",
            " |  \n",
            " |  with_mean : bool, default=True\n",
            " |      If True, center the data before scaling.\n",
            " |      This does not work (and will raise an exception) when attempted on\n",
            " |      sparse matrices, because centering them entails building a dense\n",
            " |      matrix which in common use cases is likely to be too large to fit in\n",
            " |      memory.\n",
            " |  \n",
            " |  with_std : bool, default=True\n",
            " |      If True, scale the data to unit variance (or equivalently,\n",
            " |      unit standard deviation).\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  scale_ : ndarray of shape (n_features,) or None\n",
            " |      Per feature relative scaling of the data to achieve zero mean and unit\n",
            " |      variance. Generally this is calculated using `np.sqrt(var_)`. If a\n",
            " |      variance is zero, we can't achieve unit variance, and the data is left\n",
            " |      as-is, giving a scaling factor of 1. `scale_` is equal to `None`\n",
            " |      when `with_std=False`.\n",
            " |  \n",
            " |      .. versionadded:: 0.17\n",
            " |         *scale_*\n",
            " |  \n",
            " |  mean_ : ndarray of shape (n_features,) or None\n",
            " |      The mean value for each feature in the training set.\n",
            " |      Equal to ``None`` when ``with_mean=False``.\n",
            " |  \n",
            " |  var_ : ndarray of shape (n_features,) or None\n",
            " |      The variance for each feature in the training set. Used to compute\n",
            " |      `scale_`. Equal to ``None`` when ``with_std=False``.\n",
            " |  \n",
            " |  n_features_in_ : int\n",
            " |      Number of features seen during :term:`fit`.\n",
            " |  \n",
            " |      .. versionadded:: 0.24\n",
            " |  \n",
            " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
            " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
            " |      has feature names that are all strings.\n",
            " |  \n",
            " |      .. versionadded:: 1.0\n",
            " |  \n",
            " |  n_samples_seen_ : int or ndarray of shape (n_features,)\n",
            " |      The number of samples processed by the estimator for each feature.\n",
            " |      If there are no missing samples, the ``n_samples_seen`` will be an\n",
            " |      integer, otherwise it will be an array of dtype int. If\n",
            " |      `sample_weights` are used it will be a float (if no missing data)\n",
            " |      or an array of dtype float that sums the weights seen so far.\n",
            " |      Will be reset on new calls to fit, but increments across\n",
            " |      ``partial_fit`` calls.\n",
            " |  \n",
            " |  See Also\n",
            " |  --------\n",
            " |  scale : Equivalent function without the estimator API.\n",
            " |  \n",
            " |  :class:`~sklearn.decomposition.PCA` : Further removes the linear\n",
            " |      correlation across features with 'whiten=True'.\n",
            " |  \n",
            " |  Notes\n",
            " |  -----\n",
            " |  NaNs are treated as missing values: disregarded in fit, and maintained in\n",
            " |  transform.\n",
            " |  \n",
            " |  We use a biased estimator for the standard deviation, equivalent to\n",
            " |  `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\n",
            " |  affect model performance.\n",
            " |  \n",
            " |  For a comparison of the different scalers, transformers, and normalizers,\n",
            " |  see :ref:`examples/preprocessing/plot_all_scaling.py\n",
            " |  <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> from sklearn.preprocessing import StandardScaler\n",
            " |  >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n",
            " |  >>> scaler = StandardScaler()\n",
            " |  >>> print(scaler.fit(data))\n",
            " |  StandardScaler()\n",
            " |  >>> print(scaler.mean_)\n",
            " |  [0.5 0.5]\n",
            " |  >>> print(scaler.transform(data))\n",
            " |  [[-1. -1.]\n",
            " |   [-1. -1.]\n",
            " |   [ 1.  1.]\n",
            " |   [ 1.  1.]]\n",
            " |  >>> print(scaler.transform([[2, 2]]))\n",
            " |  [[3. 3.]]\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      StandardScaler\n",
            " |      sklearn.base._OneToOneFeatureMixin\n",
            " |      sklearn.base.TransformerMixin\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, *, copy=True, with_mean=True, with_std=True)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  fit(self, X, y=None, sample_weight=None)\n",
            " |      Compute the mean and std to be used for later scaling.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The data used to compute the mean and standard deviation\n",
            " |          used for later scaling along the features axis.\n",
            " |      \n",
            " |      y : None\n",
            " |          Ignored.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Individual weights for each sample.\n",
            " |      \n",
            " |          .. versionadded:: 0.24\n",
            " |             parameter *sample_weight* support to StandardScaler.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          Fitted scaler.\n",
            " |  \n",
            " |  inverse_transform(self, X, copy=None)\n",
            " |      Scale back the data to the original representation.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The data used to scale along the features axis.\n",
            " |      copy : bool, default=None\n",
            " |          Copy the input X or not.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
            " |          Transformed array.\n",
            " |  \n",
            " |  partial_fit(self, X, y=None, sample_weight=None)\n",
            " |      Online computation of mean and std on X for later scaling.\n",
            " |      \n",
            " |      All of X is processed as a single batch. This is intended for cases\n",
            " |      when :meth:`fit` is not feasible due to very large number of\n",
            " |      `n_samples` or because X is read from a continuous stream.\n",
            " |      \n",
            " |      The algorithm for incremental mean and std is given in Equation 1.5a,b\n",
            " |      in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. \"Algorithms\n",
            " |      for computing the sample variance: Analysis and recommendations.\"\n",
            " |      The American Statistician 37.3 (1983): 242-247:\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The data used to compute the mean and standard deviation\n",
            " |          used for later scaling along the features axis.\n",
            " |      \n",
            " |      y : None\n",
            " |          Ignored.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Individual weights for each sample.\n",
            " |      \n",
            " |          .. versionadded:: 0.24\n",
            " |             parameter *sample_weight* support to StandardScaler.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          Fitted scaler.\n",
            " |  \n",
            " |  transform(self, X, copy=None)\n",
            " |      Perform standardization by centering and scaling.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix of shape (n_samples, n_features)\n",
            " |          The data used to scale along the features axis.\n",
            " |      copy : bool, default=None\n",
            " |          Copy the input X or not.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
            " |          Transformed array.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base._OneToOneFeatureMixin:\n",
            " |  \n",
            " |  get_feature_names_out(self, input_features=None)\n",
            " |      Get output feature names for transformation.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      input_features : array-like of str or None, default=None\n",
            " |          Input features.\n",
            " |      \n",
            " |          - If `input_features` is `None`, then `feature_names_in_` is\n",
            " |            used as feature names in. If `feature_names_in_` is not defined,\n",
            " |            then names are generated: `[x0, x1, ..., x(n_features_in_)]`.\n",
            " |          - If `input_features` is an array-like, then `input_features` must\n",
            " |            match `feature_names_in_` if `feature_names_in_` is defined.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      feature_names_out : ndarray of str objects\n",
            " |          Same as input features.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.base._OneToOneFeatureMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.TransformerMixin:\n",
            " |  \n",
            " |  fit_transform(self, X, y=None, **fit_params)\n",
            " |      Fit to data, then transform it.\n",
            " |      \n",
            " |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
            " |      and returns a transformed version of `X`.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          Input samples.\n",
            " |      \n",
            " |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
            " |          Target values (None for unsupervised transformations).\n",
            " |      \n",
            " |      **fit_params : dict\n",
            " |          Additional fit parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
            " |          Transformed array.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : bool, default=True\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : dict\n",
            " |          Parameter names mapped to their values.\n",
            " |  \n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |      \n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
            " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
            " |      possible to update each component of a nested object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **params : dict\n",
            " |          Estimator parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : estimator instance\n",
            " |          Estimator instance.\n",
            "\n",
            "None\n"
          ]
        }
      ]
    }
  ]
}